{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA_bot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKWZ4kMNy9Si"
      },
      "source": [
        "# necessary imports: usually i import separately in the files\n",
        "# import tensorflow as tf\n",
        "# import tensorflow_hub as hub\n",
        "!pip install transformers\n",
        "# import transformers as tfs\n",
        "# using bert from tfs i.e. BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y-8xm1RED4f"
      },
      "source": [
        "# bring in the files from github sometimes necessary once\n",
        "! git clone https://github.com/mag389/holbertonschool-machine_learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsisDajS2a2C",
        "outputId": "9f9f4245-92bf-4ad9-9945-2472f9453e9a"
      },
      "source": [
        "# bring in the files from github\n",
        "# ! git clone https://github.com/mag389/holbertonschool-machine_learning\n",
        "# install transformers into colab (only have to do the first time)\n",
        "# !pip install transformers\n",
        "# begin actual file\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import transformers\n",
        "from transformers import BertTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "\n",
        "model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
        "def question_answer(question, reference):\n",
        "    \"\"\" answers question based on reference document \"\"\"\n",
        "    tok = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "    tokenizer = BertTokenizer.from_pretrained(tok)\n",
        "    mod = \"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\"\n",
        "    # this model hub import is the very slow part\n",
        "    # model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
        "\n",
        "    question_tokens = tokenizer.tokenize(question)\n",
        "    reference_tokens = tokenizer.tokenize(reference)\n",
        "\n",
        "    tokens = ['[CLS]'] + question_tokens + \\\n",
        "        ['[SEP]'] + reference_tokens + ['[SEP]']\n",
        "    input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_mask = [1] * len(input_word_ids)\n",
        "    input_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * \\\n",
        "        (len(reference_tokens) + 1)\n",
        "    input_word_ids, input_mask, input_type_ids = map(\n",
        "            lambda t: tf.expand_dims(\n",
        "                tf.convert_to_tensor(t, dtype=tf.int32), 0),\n",
        "            (input_word_ids, input_mask, input_type_ids))\n",
        "    outputs = model([input_word_ids, input_mask, input_type_ids])\n",
        "\n",
        "    short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
        "    short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
        "    answer_tokens = tokens[short_start: short_end + 1]\n",
        "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "    if answer == \"\":\n",
        "      answer = None\n",
        "    return answer\n",
        "# exit here if you don't wnt to proceed to main\n",
        "# exit()\n",
        "# 0 main\n",
        "with open('/content/holbertonschool-machine_learning/supervised_learning/0x13-qa_bot/ZendeskArticles/PeerLearningDays.md') as f:\n",
        "    reference = f.read()\n",
        "\n",
        "print(question_answer('When are PLDs?', reference))\n",
        "print(question_answer('What are Mock Interviews?', reference))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "on - site days from 9 : 00 am to 3 : 00 pm\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnXDrnG-4jt_"
      },
      "source": [
        "# 1 loop\n",
        "if __name__ == \"__main__\":\n",
        "    words = [\"exit\", \"quit\", \"goodbye\", \"bye\"]\n",
        "    exited = 0\n",
        "    while (exited == 0):\n",
        "        print(\"Q: \", end=\"\")\n",
        "        inp = input()\n",
        "        if inp.lower() in words:\n",
        "            print(\"A: Goodbye\")\n",
        "            exited = 1\n",
        "            break\n",
        "        else:\n",
        "            print(\"A: \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqHaiO2h70ZR",
        "outputId": "36509a97-2829-486a-ab8c-1a3c00bda666"
      },
      "source": [
        "# 2 \n",
        "\n",
        "def answer_loop(reference):\n",
        "    words = [\"exit\", \"quit\", \"goodbye\", \"bye\"]\n",
        "    exited = 0\n",
        "    while (exited == 0):\n",
        "        print(\"Q: \", end=\"\")\n",
        "        inp = input()\n",
        "        if inp.lower() in words:\n",
        "            print(\"A: Goodbye\")\n",
        "            exited = 1\n",
        "            return\n",
        "        else:\n",
        "            answer = question_answer(inp, reference)\n",
        "            if answer is None or answer == \"\":\n",
        "              answer = \"Sorry, I do not understand your question.\"\n",
        "            print(\"A: \", end=\"\")\n",
        "            print(answer)\n",
        "# main section\n",
        "with open('/content/holbertonschool-machine_learning/supervised_learning/0x13-qa_bot/ZendeskArticles/PeerLearningDays.md') as f:\n",
        "    reference = f.read()\n",
        "answer_loop(reference)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q: when are plds?\n",
            "A: on - site days from 9 : 00 am to 3 : 00 pm\n",
            "Q: what are mock interviews?\n",
            "A: Sorry, I do not understand your question.\n",
            "Q: bye\n",
            "A: Goodbye\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PseV4RcFFzF",
        "outputId": "3b2626b2-c2a1-48a4-cd68-4225ae182505"
      },
      "source": [
        "# 3 semantic search\n",
        "# uncomment if not running other sections\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import transformers\n",
        "from transformers import BertTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "# import tensorflow_hub as hub\n",
        "# last import for runnign as standalone file\n",
        "\n",
        "\n",
        "use = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
        "embed = hub.load(use)\n",
        "def semantic_search(corpus_path, sentence):\n",
        "    # use = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
        "    # embed = hub.load(use)\n",
        "    embeddings = [sentence]\n",
        "\n",
        "    for name in glob.glob(corpus_path + \"/*\"):\n",
        "        with open(name) as f:\n",
        "            embeddings += [f.read()]\n",
        "\n",
        "    # print(type(embeddings))\n",
        "    # print((embeddings))\n",
        "    nums = embed(embeddings)\n",
        "    # print(nums)\n",
        "    # input()\n",
        "    corr = np.inner(nums, nums)\n",
        "    # print(corr[0, 1:])\n",
        "    # print(np.argmax(corr[0, 1:]))\n",
        "    # print(corr[0, 1:][np.argmax(corr[0, 1:])])\n",
        "    am = np.argmax(corr[0, 1:])\n",
        "    return embeddings[am + 1]\n",
        "\n",
        "# 3 main section comment out if only creating function for other uses\n",
        "location = '/content/holbertonschool-machine_learning/supervised_learning/0x13-qa_bot/ZendeskArticles'\n",
        "print(semantic_search(location, 'When are PLDs?'))\n",
        "             "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PLD Overview\n",
            "Peer Learning Days (PLDs) are a time for you and your peers to ensure that each of you understands the concepts you've encountered in your projects, as well as a time for everyone to collectively grow in technical, professional, and soft skills. During PLD, you will collaboratively review prior projects with a group of cohort peers.\n",
            "PLD Basics\n",
            "PLDs are mandatory on-site days from 9:00 AM to 3:00 PM. If you cannot be present or on time, you must use a PTO. \n",
            "No laptops, tablets, or screens are allowed until all tasks have been whiteboarded and understood by the entirety of your group. This time is for whiteboarding, dialogue, and active peer collaboration. After this, you may return to computers with each other to pair or group program. \n",
            "Peer Learning Days are not about sharing solutions. This doesn't empower peers with the ability to solve problems themselves! Peer learning is when you share your thought process, whether through conversation, whiteboarding, debugging, or live coding. \n",
            "When a peer has a question, rather than offering the solution, ask the following:\n",
            "\"How did you come to that conclusion?\"\n",
            "\"What have you tried?\"\n",
            "\"Did the man page give you a lead?\"\n",
            "\"Did you think about this concept?\"\n",
            "Modeling this form of thinking for one another is invaluable and will strengthen your entire cohort.\n",
            "Your ability to articulate your knowledge is a crucial skill and will be required to succeed during technical interviews and through your career. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq6rDVbfGctS",
        "outputId": "66d466a8-2779-4615-f6e8-e45a728e5d0e"
      },
      "source": [
        "# 4 the final qa: \n",
        "model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
        "def question_answer(coprus_path):\n",
        "    \"\"\" answer questions from whole corpus \"\"\"\n",
        "    words = [\"exit\", \"quit\", \"goodbye\", \"bye\"]\n",
        "    exited = 0\n",
        "    while (exited == 0):\n",
        "        print(\"Q: \", end=\"\")\n",
        "        inp = input()\n",
        "        if inp.lower() in words:\n",
        "            print(\"A: Goodbye\")\n",
        "            exited = 1\n",
        "            return\n",
        "        else:\n",
        "            refer = semantic_search(coprus_path, inp)\n",
        "            answer = question_answer1(inp, refer)\n",
        "            print(\"A: \", end=\"\")\n",
        "            print(answer)\n",
        "# helper functoin\n",
        "def question_answer1(question, reference):\n",
        "    \"\"\" answers question based on reference document \"\"\"\n",
        "    tok = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "    tokenizer = BertTokenizer.from_pretrained(tok)\n",
        "    mod = \"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\"\n",
        "    # model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
        "\n",
        "    question_tokens = tokenizer.tokenize(question)\n",
        "    reference_tokens = tokenizer.tokenize(reference)\n",
        "\n",
        "    tokens = ['[CLS]'] + question_tokens + \\\n",
        "        ['[SEP]'] + reference_tokens + ['[SEP]']\n",
        "    input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_mask = [1] * len(input_word_ids)\n",
        "    input_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * \\\n",
        "        (len(reference_tokens) + 1)\n",
        "    input_word_ids, input_mask, input_type_ids = map(\n",
        "            lambda t: tf.expand_dims(\n",
        "                tf.convert_to_tensor(t, dtype=tf.int32), 0),\n",
        "            (input_word_ids, input_mask, input_type_ids))\n",
        "    outputs = model([input_word_ids, input_mask, input_type_ids])\n",
        "\n",
        "    short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
        "    short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
        "    answer_tokens = tokens[short_start: short_end + 1]\n",
        "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "    if answer == \"\":\n",
        "        answer = None\n",
        "    return answer\n",
        "\n",
        "# and associated main\n",
        "\n",
        "\n",
        "question_answer('/content/holbertonschool-machine_learning/supervised_learning/0x13-qa_bot/ZendeskArticles')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q: when are plds?\n",
            "A: on - site days from 9 : 00 am to 3 : 00 pm\n",
            "Q: what are mock interviews?\n",
            "A: help you train for technical interviews\n",
            "Q: bye\n",
            "A: Goodbye\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}