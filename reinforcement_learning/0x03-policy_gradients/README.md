# Policy gradients

An alternative to temporal learning is using policy gradients.  There are
many advantages to a policy gradient approach. here we view and implemeent them.

## Resources:
- full RL book: http://incompleteideas.net/book/RLbook2018.pdf

- David silvers Lectures (particularyly 6 and 7 for this topic)
https://www.davidsilver.uk/teaching/

- the slides from relevant lecture: https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf

- https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/

- https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html

- https://aerinykim.medium.com/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d

- monte carlo methods for learning: https://towardsdatascience.com/my-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032
  - (not policy adjustment. previous work, action/value adjustment)

- brief holberton explanation: https://www.youtube.com/watch?v=PxmtzoW4GDA&t=19s
  - (again last project)

#### For understanding how to calculate policy gradient:
* https://medium.com/samkirkiles/reinforce-policy-gradients-from-scratch-in-numpy-6a09ae0dfe12
  * for the more code based approach
* https://math.stackexchange.com/questions/2013050/log-of-softmax-function-derivative/2340848#2340848
  * for the mathematical derivation

#### Related but unecessary:
* https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Cartpole/Cartpole%20REINFORCE%20Monte%20Carlo%20Policy%20Gradients.ipynb
  * similar cartpole problem, but with tensorflow
* 
